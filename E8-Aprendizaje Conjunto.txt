APRENDIZAJE CONJUNTO
El aprendizaje conjunto se ha posicionado como una de las técnicas más exitosas en machine learning debido a su capacidad de combinar varios modelos con el propósito de mejorar los resultados del aprendizaje automático, permitiendo la producción de un mejor rendimiento predictivo en comparación con un solo modelo.
El objetivo de cualquier problema de aprendizaje automático es encontrar un modelo único que pueda predecir mejor nuestro resultado deseado, disminuyendo la varianza, y el sesgo. En lugar de hacer un modelo y esperar que este sea el mejor predictor que podamos hacer, los métodos de conjunto tienen en cuenta una gran cantidad de modelos, los promedian y producen un modelo final.
Algunas de las técnicas de aprendizaje conjunto más usadas son:
Bagging: Como se mencionó anteriormente, el Bagging nos ayuda a reducir el error de la varianza, no a mejorar la fuerza predictiva del modelo. Dada una muestra de datos, se extraen varias submuestras, con esto se forma un árbol de decisión en cada una de estas, para luego utilizar un algoritmo que se agregará a los árboles de decisión con el fin último de formar el predictor más eficiente.
Boosting: A grandes rasgos, esta es una técnica iterativa que ajusta el peso de una observación basada en la última clasificación. Si una observación se clasificó incorrectamente, intenta aumentar el peso de esta observación y viceversa. El aumento en general disminuye el error de sesgo y construye modelos predictivos sólidos.
Stacking: Esta técnica es similar a boosting, también aplica varios modelos a sus datos originales. Sin embargo, la diferencia es que no tiene solo una fórmula empírica para su función de ponderación, sino que introduce un meta-nivel y utiliza otro modelo / enfoque para determinar qué modelos funcionan bien y cuales no, teniendo en cuenta los datos de entrada.
