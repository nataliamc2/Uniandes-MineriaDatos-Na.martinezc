¿NECESITAMOS CIENTOS DE CLASIFICADORES PARA RESOLVER PROBLEMAS DE CLASIFICACIÓN EN EL MUNDO REAL?

Comenzaré respondiendo la pregunta con la que Manuel Fernandez, Eva Cernadas y Senen Barro dan inicio a su artículo. La respuesta es NO, no necesitamos cientos de clasificadores para resolver problemas de clasificación en el mundo real. Estos autores lograron demostrar a través de una evaluación exhaustiva de 179 clasificadores pertenecientes a una amplia colección de 17 familias en toda la base de datos de clasificación de aprendizaje automático de UCI, que los mejores resultados se obtienen mediante el Parallel Random Forest (parRF t), implementado en R con caret, tuning el parámetro mtry. Según ellos, el parRF t alcanza en promedio el 94.1% de la precisión máxima sobre todos los conjuntos de datos, y supera el 90% de la precisión máxima en 102 de los 121 conjuntos de datos. Su precisión promedio en todos los conjuntos de datos es del 82.0%, mientras que la precisión promedio máxima (lograda por el mejor clasificador para cada conjunto de datos) es del 86.9%. Por otro lado, acotan afirmando que el Random Forest en R y tuning con caret (rf t) es ligeramente peor (93.6% de la precisión máxima), aunque alcanza una precisión promedio ligeramente mayor (82.3%) que el parRF t.
A mi parecer, lo más relevante de este estudio, es que nos permite entender la importancia de ajustar cualquier modelo que se lleve a cabo, pero más allá de esto, el artículo está encaminado a cuantificar el impacto de ajustar los parámetros, lo que nos proporciona información valiosa a la hora de comenzar un proyecto y tomar la difícil decisión de qué clasificador usar o cual se ajustará mejor a los datos con el propósito de acortar el camino a la meta y de esta manera llegar a los resultados deseados.
