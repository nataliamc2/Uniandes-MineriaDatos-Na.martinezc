{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 9\n",
    "\n",
    "## Mashable news stories analysis\n",
    "\n",
    "Predicting if a news story is going to be popular\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>url</th>\n",
       "      <th>timedelta</th>\n",
       "      <th>n_tokens_title</th>\n",
       "      <th>n_tokens_content</th>\n",
       "      <th>n_unique_tokens</th>\n",
       "      <th>n_non_stop_words</th>\n",
       "      <th>n_non_stop_unique_tokens</th>\n",
       "      <th>num_hrefs</th>\n",
       "      <th>num_self_hrefs</th>\n",
       "      <th>num_imgs</th>\n",
       "      <th>...</th>\n",
       "      <th>min_positive_polarity</th>\n",
       "      <th>max_positive_polarity</th>\n",
       "      <th>avg_negative_polarity</th>\n",
       "      <th>min_negative_polarity</th>\n",
       "      <th>max_negative_polarity</th>\n",
       "      <th>title_subjectivity</th>\n",
       "      <th>title_sentiment_polarity</th>\n",
       "      <th>abs_title_subjectivity</th>\n",
       "      <th>abs_title_sentiment_polarity</th>\n",
       "      <th>Popular</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>http://mashable.com/2014/12/10/cia-torture-rep...</td>\n",
       "      <td>28.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>188.0</td>\n",
       "      <td>0.732620</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.844262</td>\n",
       "      <td>5.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.200000</td>\n",
       "      <td>0.80</td>\n",
       "      <td>-0.487500</td>\n",
       "      <td>-0.60</td>\n",
       "      <td>-0.250000</td>\n",
       "      <td>0.9</td>\n",
       "      <td>0.8</td>\n",
       "      <td>0.4</td>\n",
       "      <td>0.8</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>http://mashable.com/2013/10/18/bitlock-kicksta...</td>\n",
       "      <td>447.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>297.0</td>\n",
       "      <td>0.653199</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.815789</td>\n",
       "      <td>9.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.160000</td>\n",
       "      <td>0.50</td>\n",
       "      <td>-0.135340</td>\n",
       "      <td>-0.40</td>\n",
       "      <td>-0.050000</td>\n",
       "      <td>0.1</td>\n",
       "      <td>-0.1</td>\n",
       "      <td>0.4</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>http://mashable.com/2013/07/24/google-glass-po...</td>\n",
       "      <td>533.0</td>\n",
       "      <td>11.0</td>\n",
       "      <td>181.0</td>\n",
       "      <td>0.660377</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.775701</td>\n",
       "      <td>4.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.136364</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.3</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.2</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>http://mashable.com/2013/11/21/these-are-the-m...</td>\n",
       "      <td>413.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>781.0</td>\n",
       "      <td>0.497409</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.677350</td>\n",
       "      <td>10.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>1.00</td>\n",
       "      <td>-0.195701</td>\n",
       "      <td>-0.40</td>\n",
       "      <td>-0.071429</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>http://mashable.com/2014/02/11/parking-ticket-...</td>\n",
       "      <td>331.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>177.0</td>\n",
       "      <td>0.685714</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.830357</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>0.55</td>\n",
       "      <td>-0.175000</td>\n",
       "      <td>-0.25</td>\n",
       "      <td>-0.100000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 61 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 url  timedelta  \\\n",
       "0  http://mashable.com/2014/12/10/cia-torture-rep...       28.0   \n",
       "1  http://mashable.com/2013/10/18/bitlock-kicksta...      447.0   \n",
       "2  http://mashable.com/2013/07/24/google-glass-po...      533.0   \n",
       "3  http://mashable.com/2013/11/21/these-are-the-m...      413.0   \n",
       "4  http://mashable.com/2014/02/11/parking-ticket-...      331.0   \n",
       "\n",
       "   n_tokens_title  n_tokens_content  n_unique_tokens  n_non_stop_words  \\\n",
       "0             9.0             188.0         0.732620               1.0   \n",
       "1             7.0             297.0         0.653199               1.0   \n",
       "2            11.0             181.0         0.660377               1.0   \n",
       "3            12.0             781.0         0.497409               1.0   \n",
       "4             8.0             177.0         0.685714               1.0   \n",
       "\n",
       "   n_non_stop_unique_tokens  num_hrefs  num_self_hrefs  num_imgs   ...     \\\n",
       "0                  0.844262        5.0             1.0       1.0   ...      \n",
       "1                  0.815789        9.0             4.0       1.0   ...      \n",
       "2                  0.775701        4.0             3.0       1.0   ...      \n",
       "3                  0.677350       10.0             3.0       1.0   ...      \n",
       "4                  0.830357        3.0             2.0       1.0   ...      \n",
       "\n",
       "   min_positive_polarity  max_positive_polarity  avg_negative_polarity  \\\n",
       "0               0.200000                   0.80              -0.487500   \n",
       "1               0.160000                   0.50              -0.135340   \n",
       "2               0.136364                   1.00               0.000000   \n",
       "3               0.100000                   1.00              -0.195701   \n",
       "4               0.100000                   0.55              -0.175000   \n",
       "\n",
       "   min_negative_polarity  max_negative_polarity  title_subjectivity  \\\n",
       "0                  -0.60              -0.250000                 0.9   \n",
       "1                  -0.40              -0.050000                 0.1   \n",
       "2                   0.00               0.000000                 0.3   \n",
       "3                  -0.40              -0.071429                 0.0   \n",
       "4                  -0.25              -0.100000                 0.0   \n",
       "\n",
       "   title_sentiment_polarity  abs_title_subjectivity  \\\n",
       "0                       0.8                     0.4   \n",
       "1                      -0.1                     0.4   \n",
       "2                       1.0                     0.2   \n",
       "3                       0.0                     0.5   \n",
       "4                       0.0                     0.5   \n",
       "\n",
       "   abs_title_sentiment_polarity  Popular  \n",
       "0                           0.8        1  \n",
       "1                           0.1        0  \n",
       "2                           1.0        0  \n",
       "3                           0.0        0  \n",
       "4                           0.0        0  \n",
       "\n",
       "[5 rows x 61 columns]"
      ]
     },
     "execution_count": 190,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "url = 'https://raw.githubusercontent.com/albahnsen/PracticalMachineLearningClass/master/datasets/mashable.csv'\n",
    "train_df = pd.read_csv(url, index_col=0)\n",
    "train_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(6000, 61)"
      ]
     },
     "execution_count": 191,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = train_df.drop(['url', 'Popular'], axis=1)\n",
    "y = train_df['Popular']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5"
      ]
     },
     "execution_count": 193,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train/test split\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 9.1\n",
    "\n",
    "Estimate a Decision Tree Classifier and a Logistic Regresion\n",
    "\n",
    "Evaluate using the following metrics:\n",
    "* Accuracy\n",
    "* F1-Score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DecisionTreeClassifier(class_weight=None, criterion='gini', max_depth=None,\n",
       "            max_features=None, max_leaf_nodes=None,\n",
       "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "            min_samples_leaf=1, min_samples_split=2,\n",
       "            min_weight_fraction_leaf=0.0, presort=False, random_state=None,\n",
       "            splitter='best')"
      ]
     },
     "execution_count": 195,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Decision Tree Classifier \n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "treeclf = DecisionTreeClassifier()\n",
    "treeclf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy Score: 0.551\n",
      "F1 Score:0.552\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn import metrics\n",
    "treeclf.score(X_test, y_test)\n",
    "y_pred=treeclf.predict(X_test)\n",
    "print(f\"Accuracy Score: {round(accuracy_score(y_test, y_pred),3)}\")\n",
    "print(f\"F1 Score:{round(f1_score(y_pred, y_test),3)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy Score: 0.627\n",
      "F1 Score:0.61\n"
     ]
    }
   ],
   "source": [
    "#Logistic Regression\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "logreg = LogisticRegression(C=1e9, solver='liblinear', multi_class='ovr')\n",
    "logreg.fit(X_train, y_train)\n",
    "y_pred_logreg = logreg.predict(X_test)\n",
    "print(f\"Accuracy Score: {round(accuracy_score(y_test, y_pred_logreg),3)}\")\n",
    "print(f\"F1 Score:{round(f1_score(y_pred_logreg, y_test),3)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 9.2\n",
    "\n",
    "Estimate 300 bagged samples\n",
    "\n",
    "Estimate the following set of classifiers:\n",
    "\n",
    "* 100 Decision Trees where max_depth=None\n",
    "* 100 Decision Trees where max_depth=2\n",
    "* 100 Logistic Regressions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([3582, 3454, 1346, ..., 3447, 4777, 2770]),\n",
       " array([4620, 2546,  261, ...,  720, 2868, 4917]),\n",
       " array([5307,  594,  925, ...,  615, 1490, 3208]),\n",
       " array([4288, 2101, 4399, ..., 3458, 5177, 5219]),\n",
       " array([3131, 3822, 2323, ..., 5651,  600, 1805]),\n",
       " array([ 138, 1400,  803, ..., 2648, 4349, 5664]),\n",
       " array([5238,  630,  273, ..., 4814, 5828,  881]),\n",
       " array([2123, 4123, 5343, ..., 1651, 4474, 5586]),\n",
       " array([ 746, 4683,  513, ...,  710, 3553, 2096]),\n",
       " array([2467, 5108, 5052, ..., 3728, 5389, 4002]),\n",
       " array([2781, 1976, 4193, ..., 5248, 2939, 4780]),\n",
       " array([4240, 5109, 5901, ..., 1458, 2370, 2017]),\n",
       " array([ 624, 1531, 4806, ..., 4399, 1262, 5119]),\n",
       " array([2085, 3779, 1301, ..., 5613, 2635, 2590]),\n",
       " array([4143, 4896, 3125, ...,  953, 1640, 3983]),\n",
       " array([5962, 4997, 1412, ..., 3863, 3002,  436]),\n",
       " array([4868, 3275, 5541, ..., 3911, 1983,  124]),\n",
       " array([2633, 3671, 5680, ..., 2780, 1403, 4215]),\n",
       " array([2760,  765,  115, ..., 5250, 5560, 1993]),\n",
       " array([4377, 5845, 3747, ..., 4118, 4412,   94]),\n",
       " array([3952, 5559, 4232, ..., 4313, 2715, 3386]),\n",
       " array([1054, 2798, 2734, ..., 2001, 4357, 4359]),\n",
       " array([2664, 2877, 4482, ..., 5931, 3971, 5523]),\n",
       " array([ 938, 1219, 3406, ..., 5282, 5919, 3595]),\n",
       " array([3596, 2457,  358, ..., 5397,  748,  579]),\n",
       " array([1066, 2365, 2569, ...,  281, 5191, 3945]),\n",
       " array([4072, 2087, 2654, ..., 2151,  921, 4909]),\n",
       " array([5866, 3771, 4066, ..., 4288, 3335, 3608]),\n",
       " array([2782,  510, 5689, ...,  863, 5100, 5123]),\n",
       " array([2932, 4237, 1706, ..., 4071, 2847, 1041]),\n",
       " array([3124, 4493,  986, ..., 2297, 4944,   30]),\n",
       " array([2750, 3088,  160, ..., 2229,  112, 3189]),\n",
       " array([2906, 1050, 3870, ...,  578, 1774, 4498]),\n",
       " array([4843,  889, 3181, ..., 4704, 4951, 1381]),\n",
       " array([1110, 2066, 4060, ..., 2069, 4790,  946]),\n",
       " array([ 153, 4775, 1661, ..., 3277, 5781,  299]),\n",
       " array([3480, 5194, 5270, ..., 2547,  625, 5425]),\n",
       " array([5234, 1614,   58, ..., 5287, 3252, 3987]),\n",
       " array([2328, 4810, 5361, ..., 1123, 3309, 4825]),\n",
       " array([3345, 1292, 3469, ..., 4721, 5887, 2905]),\n",
       " array([5848, 2467, 3801, ...,  705, 3048, 3015]),\n",
       " array([4429,  203,  416, ..., 1296, 2323, 1416]),\n",
       " array([ 140, 2682, 1085, ..., 4865, 3910, 3308]),\n",
       " array([2563, 5239, 4502, ..., 3651, 2093, 5413]),\n",
       " array([5356, 3437, 1910, ..., 1776, 1548, 2657]),\n",
       " array([5210,  350, 2109, ..., 4179, 4266, 4682]),\n",
       " array([2589, 4046, 5661, ..., 2587, 4118, 3246]),\n",
       " array([5405, 3503, 1482, ..., 2656, 4434, 4556]),\n",
       " array([2043, 5454, 4790, ..., 1407, 2174, 1607]),\n",
       " array([2385, 2582, 2765, ..., 5286, 3794, 5602]),\n",
       " array([2895, 5161, 1657, ..., 4256, 1516, 3608]),\n",
       " array([2034, 5650, 5499, ...,  426, 5842, 4860]),\n",
       " array([3074, 3652, 5140, ..., 3361, 5823, 1563]),\n",
       " array([4929, 4648, 1450, ..., 3610, 3492, 1698]),\n",
       " array([1651, 5634, 5042, ..., 1050, 3000,   99]),\n",
       " array([2603, 3088, 2964, ..., 4879,  259, 2141]),\n",
       " array([1451,  681, 3000, ..., 3063, 4792, 1485]),\n",
       " array([ 569, 1297, 4690, ...,  382, 4322, 3877]),\n",
       " array([1759, 2919, 2357, ..., 1546, 1301,  966]),\n",
       " array([ 812, 5049,  442, ..., 3725, 2109, 4056]),\n",
       " array([5810, 2651, 5118, ..., 4344,  721, 4082]),\n",
       " array([4327, 2189,  241, ..., 4797, 2384, 4123]),\n",
       " array([4566, 3341,  540, ..., 4234, 4043,  847]),\n",
       " array([1442,  819, 3844, ..., 2249,  699, 1241]),\n",
       " array([4437, 5215,  403, ..., 1978, 4858, 2247]),\n",
       " array([4439, 5788,  927, ..., 2757, 3973, 3250]),\n",
       " array([5016, 1334, 4207, ...,   13, 4523,  776]),\n",
       " array([1548, 3857,  768, ..., 5081, 2854, 3260]),\n",
       " array([2059, 1014, 4196, ..., 4316, 2655, 5581]),\n",
       " array([4910, 5827, 3035, ..., 3293, 1748,  218]),\n",
       " array([2350,  720, 3239, ..., 5114, 3082,  554]),\n",
       " array([4136, 2628, 2131, ..., 1079, 1134, 2228]),\n",
       " array([5045, 5780, 2559, ..., 1223, 1164, 4234]),\n",
       " array([3104, 3816, 2731, ...,  187, 5893, 1148]),\n",
       " array([5321, 1895, 2260, ..., 2341, 4775, 4526]),\n",
       " array([1036, 3896,  855, ..., 2684, 3024, 1206]),\n",
       " array([1893, 5940, 1336, ..., 4373, 5597, 2416]),\n",
       " array([4793, 1581, 5558, ..., 2078, 1543, 2798]),\n",
       " array([1173, 3398, 1896, ..., 2982, 5772, 1055]),\n",
       " array([1589, 5328, 2789, ..., 1743,  155,  450]),\n",
       " array([5790, 2133, 5411, ..., 1976, 4935, 2825]),\n",
       " array([3738, 3148,  560, ..., 4928, 5361, 3283]),\n",
       " array([3680, 5584, 3949, ..., 4769, 2171, 1603]),\n",
       " array([ 745, 4904,  465, ..., 4187,  766, 1418]),\n",
       " array([2389, 1150, 1827, ..., 4642, 2566, 1009]),\n",
       " array([1998,  125, 2333, ..., 5414, 5304, 1143]),\n",
       " array([4968, 5747, 1157, ..., 2581,  570, 4986]),\n",
       " array([5546, 4016, 3612, ..., 5503, 4310, 4828]),\n",
       " array([1641, 1841, 2874, ..., 5627, 4764, 3144]),\n",
       " array([ 913, 4558, 1235, ..., 1312,  970, 2612]),\n",
       " array([ 882, 2642, 2250, ..., 5699, 1556,  978]),\n",
       " array([1105, 4121, 3552, ..., 1983,  321, 4789]),\n",
       " array([4632,  989, 3300, ..., 4130, 1614, 3898]),\n",
       " array([1266, 3795, 2590, ..., 1359, 2335, 3596]),\n",
       " array([4922, 3163,  579, ..., 4029, 2731, 4343]),\n",
       " array([3589, 1111, 2989, ..., 3376, 1910, 4720]),\n",
       " array([ 908, 3287,   84, ..., 2649, 1184, 2423]),\n",
       " array([4787,  751, 2432, ..., 1175, 5254, 1832]),\n",
       " array([ 103, 3902,  588, ..., 4483, 3348,  485]),\n",
       " array([ 720, 1109, 4192, ..., 3759, 4353,   93])]"
      ]
     },
     "execution_count": 198,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# set a seed for reproducibility\n",
    "np.random.seed(123)\n",
    "\n",
    "n_samples = train_df.shape[0]\n",
    "n_B = 100\n",
    "\n",
    "# create ten bootstrap samples (will be used to select rows from the DataFrame)\n",
    "samples = [np.random.choice(a=n_samples, size=n_samples, replace=True) for _ in range(1, n_B +1 )]\n",
    "samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([3005, 2205, 5346, ...,  700, 1855,  828]),\n",
       " array([1933, 2512, 2845, ..., 3736, 1097, 3027]),\n",
       " array([3415, 5428, 2048, ..., 4098, 5701, 2674]),\n",
       " array([1878, 3948, 1726, ..., 3904, 1051, 3432]),\n",
       " array([ 394, 3206,  459, ..., 2624, 2862, 5873]),\n",
       " array([5247, 1212, 1842, ...,  971, 1408,  780]),\n",
       " array([5178,  303, 2671, ...,  378, 3104, 1919]),\n",
       " array([5386, 2725,  116, ..., 1359,  457, 2170]),\n",
       " array([ 833, 4944,  933, ..., 2659, 5021,  373]),\n",
       " array([ 226, 4979,  515, ..., 1489,  949, 4391]),\n",
       " array([2261, 2051, 4722, ...,  697, 2525, 3145]),\n",
       " array([2701, 4694, 4501, ...,  293, 4122,  696]),\n",
       " array([  84, 1316, 1861, ..., 1372,  441, 2139]),\n",
       " array([2101, 3667, 4960, ...,  160, 3161, 1832]),\n",
       " array([3289, 3969, 2728, ..., 4658, 4961, 5461]),\n",
       " array([2900, 1289, 4265, ..., 5190, 4762, 5793]),\n",
       " array([3398,  633, 4648, ..., 3461, 4238, 1051]),\n",
       " array([1378, 3857, 2890, ..., 1390, 5039, 5439]),\n",
       " array([5235, 4530, 1477, ..., 2206, 1985, 4265]),\n",
       " array([5499,  865, 5542, ..., 5673,  412, 4438]),\n",
       " array([ 363, 5339, 2150, ...,   49, 3592, 5745]),\n",
       " array([1524, 4019, 3482, ..., 1207, 5820,  503]),\n",
       " array([4332,  684,  584, ..., 4006, 4744, 2321]),\n",
       " array([2700, 3972, 1395, ..., 3707, 5116, 4281]),\n",
       " array([4374, 3419, 5485, ...,  586, 5822, 3704]),\n",
       " array([2414, 2454, 5005, ..., 1915, 1382, 5565]),\n",
       " array([1159,  872, 1343, ..., 5841, 2427, 3119]),\n",
       " array([3659, 4720, 1324, ..., 5323, 2611, 5836]),\n",
       " array([3095,  486, 3752, ..., 5932, 3431, 4336]),\n",
       " array([5465, 4424, 1661, ..., 1417, 5858, 2826]),\n",
       " array([4955,  281, 1726, ..., 2476, 5892, 4469]),\n",
       " array([4238, 1105, 4297, ..., 1763, 4663, 1439]),\n",
       " array([ 552, 3325, 2902, ..., 3096, 4232, 1642]),\n",
       " array([2868,   27, 3236, ..., 3490,    6, 1000]),\n",
       " array([ 934, 1113,   81, ...,  934,  869,  442]),\n",
       " array([1082, 4588, 3538, ...,  839, 4645, 3718]),\n",
       " array([3321,  976, 5076, ...,  585, 3237, 5597]),\n",
       " array([ 474, 1801,  717, ...,  995, 2935, 3678]),\n",
       " array([1832, 4269, 3245, ..., 2955,  865,   76]),\n",
       " array([3514, 3645,  998, ..., 5297, 5670, 1943]),\n",
       " array([1281, 2888, 4958, ..., 3513, 1632, 4280]),\n",
       " array([5132, 2075, 1958, ..., 4037, 5391, 2671]),\n",
       " array([5460, 5668,   49, ..., 4802, 1388, 4169]),\n",
       " array([5827, 1211,  388, ..., 2879, 3855,  415]),\n",
       " array([5643, 2570, 5484, ..., 2950, 5269, 2560]),\n",
       " array([2369, 3655, 3766, ..., 3845, 1535, 5808]),\n",
       " array([3029, 4665, 1892, ..., 2886, 4557, 5155]),\n",
       " array([4147, 3123, 2438, ..., 5734,  885, 1291]),\n",
       " array([3508, 3761, 1567, ..., 3425, 5655, 1197]),\n",
       " array([5585, 1652, 5830, ..., 2345, 2371, 3035]),\n",
       " array([2376,  509, 2671, ..., 5901,  620, 4631]),\n",
       " array([1496, 5632, 1232, ..., 4759, 4192,  658]),\n",
       " array([2202, 2472, 2288, ...,  800,  471, 4412]),\n",
       " array([5281, 4827,  609, ...,   89, 3274, 3155]),\n",
       " array([2269, 1632, 5665, ..., 2359, 3908, 3251]),\n",
       " array([ 643, 5557,  154, ..., 4045,  304,  901]),\n",
       " array([4925, 1877,  751, ..., 3196, 3734, 1290]),\n",
       " array([2493, 3479, 5932, ..., 1135, 4004, 1346]),\n",
       " array([1800, 3474, 3342, ...,  931,  987, 1486]),\n",
       " array([1770, 5853, 2545, ...,  974, 4113, 3885]),\n",
       " array([5340, 3197, 2208, ..., 2724, 2119, 3886]),\n",
       " array([5697,  824, 4146, ..., 1670, 5480, 2924]),\n",
       " array([5920, 5988, 3614, ..., 4860, 2932, 2569]),\n",
       " array([3479, 2292, 5877, ..., 4039, 2127, 1837]),\n",
       " array([1848, 1559, 1660, ..., 5576,  568, 3736]),\n",
       " array([3189,  987, 2159, ..., 5581, 3119, 4683]),\n",
       " array([1454, 4059, 4138, ..., 2304, 4921, 3315]),\n",
       " array([3646, 4884, 1407, ...,   43, 2406, 4582]),\n",
       " array([1200, 3364, 1467, ..., 1705, 2991, 2356]),\n",
       " array([2416, 5297,  873, ..., 5133,  757, 1828]),\n",
       " array([3129, 3109,  481, ..., 4358, 3714, 3929]),\n",
       " array([2762,    3, 4912, ..., 4817, 3278, 1835]),\n",
       " array([5851,  974, 2514, ...,  405, 5598, 2523]),\n",
       " array([1239, 1631, 3130, ..., 2574, 3232, 3899]),\n",
       " array([4922, 1049, 5014, ..., 2260,  409,  931]),\n",
       " array([2898, 4759, 1514, ..., 2887, 4195, 2954]),\n",
       " array([ 885, 5900, 1764, ..., 4586, 2550,  340]),\n",
       " array([3007, 5878, 2804, ..., 5050, 3136, 1695]),\n",
       " array([ 936, 3269, 1314, ..., 3069, 4514, 5389]),\n",
       " array([3732,  771, 4117, ..., 4012, 2243, 3353]),\n",
       " array([5935, 1351,  947, ..., 5927, 2372, 2747]),\n",
       " array([3203, 2579, 5891, ..., 5466, 5054, 1720]),\n",
       " array([1439, 1219, 2929, ..., 5435,  419, 4260]),\n",
       " array([4238, 4739, 5720, ..., 2873, 1854, 1778]),\n",
       " array([3174, 2141,  456, ..., 3620,  327, 3144]),\n",
       " array([5953, 5935, 3253, ..., 5739, 1836, 5487]),\n",
       " array([ 888, 2759,  544, ..., 3400, 4749, 1872]),\n",
       " array([ 933, 5192, 1150, ..., 2840, 1517, 3370]),\n",
       " array([5083, 1347, 4076, ..., 2611, 2957, 4930]),\n",
       " array([3329, 2969, 4238, ..., 3049, 3454, 5257]),\n",
       " array([4711, 5765,   50, ..., 2856, 5944, 4791]),\n",
       " array([5320,  793, 4328, ..., 3550, 5427, 3239]),\n",
       " array([5586, 3880,  530, ...,  517, 1036, 1448]),\n",
       " array([5932, 1802, 1398, ...,  394, 3850, 4846]),\n",
       " array([5778, 3530, 3459, ..., 2691, 3819, 4211]),\n",
       " array([5534,  437, 3483, ...,  469, 1809, 5082]),\n",
       " array([5714, 1906, 3770, ..., 2160, 2471, 3729]),\n",
       " array([1061,  277, 4890, ...,  466, 1639, 4171]),\n",
       " array([1975, 5233, 3644, ..., 1439, 3032, 5483]),\n",
       " array([5526, 3808,  717, ..., 2318, 5695,  659])]"
      ]
     },
     "execution_count": 199,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# set a seed for reproducibility\n",
    "np.random.seed(125)\n",
    "\n",
    "n_samples1 = train_df.shape[0]\n",
    "n_B1 = 100\n",
    "\n",
    "# create ten bootstrap samples (will be used to select rows from the DataFrame)\n",
    "samples1 = [np.random.choice(a=n_samples, size=n_samples, replace=True) for _ in range(1, n_B +1 )]\n",
    "samples1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([1200, 4806, 3586, ..., 1407, 3443,  840]),\n",
       " array([5704,  517, 5344, ..., 5328, 1341, 3788]),\n",
       " array([3259, 2367, 1487, ..., 1098, 3150, 3869]),\n",
       " array([4694, 4642,  115, ..., 4676, 5905, 5148]),\n",
       " array([4025, 3690, 1399, ..., 1458, 2641,  592]),\n",
       " array([ 746, 3223, 4089, ..., 3102, 5181, 3400]),\n",
       " array([3673,  965, 4739, ..., 4882, 2794, 3399]),\n",
       " array([1898, 5776, 3012, ..., 4932, 4865, 2830]),\n",
       " array([2254, 4181, 1184, ...,  378, 5849, 2294]),\n",
       " array([2493,  235,  455, ..., 3253, 2078, 4527]),\n",
       " array([ 708,  988, 3108, ..., 5338, 2870, 1965]),\n",
       " array([1705, 4897, 4371, ..., 2435, 1844, 1288]),\n",
       " array([2239, 1104,  247, ..., 2821, 3150, 5608]),\n",
       " array([ 136, 4493,  747, ...,  990, 1478, 3612]),\n",
       " array([ 226, 5672, 4269, ..., 5753, 2472, 4663]),\n",
       " array([1537, 5168, 4158, ..., 1961, 3022, 3024]),\n",
       " array([3098, 1912, 2674, ..., 2319, 3097, 5738]),\n",
       " array([4880, 1064, 2924, ..., 5803, 3507, 1962]),\n",
       " array([4358, 4861,  479, ...,  871, 2854, 2896]),\n",
       " array([1356, 5884,  586, ..., 4583, 3484, 3351]),\n",
       " array([3534, 3246, 1224, ..., 1048, 4764, 4965]),\n",
       " array([ 608, 3122, 2362, ...,   60, 5823, 2431]),\n",
       " array([3683,  342, 2782, ..., 3529, 1242, 1109]),\n",
       " array([ 658, 2848, 3775, ..., 1660, 4501, 3571]),\n",
       " array([1275, 4163, 2693, ..., 1654, 1660, 4987]),\n",
       " array([5011,  935, 1217, ..., 3380,  420, 5662]),\n",
       " array([3540, 2261, 4020, ..., 1656, 5689, 4434]),\n",
       " array([3441, 1847, 1111, ..., 5699, 3037, 1251]),\n",
       " array([ 558, 4930, 1173, ..., 1603, 3510, 3340]),\n",
       " array([ 772, 5873, 2431, ..., 5657, 4312, 5957]),\n",
       " array([2728, 2886,  763, ..., 5914, 4682, 3372]),\n",
       " array([4933, 5295, 4777, ..., 1295, 5638, 2009]),\n",
       " array([3855, 5454, 5470, ..., 3896, 5868, 3519]),\n",
       " array([1809, 3141, 4362, ..., 4029,  201, 2087]),\n",
       " array([2807, 3390, 2463, ..., 2065, 5339, 4906]),\n",
       " array([2824, 5657, 3084, ..., 1272, 5281, 4798]),\n",
       " array([1671, 3518, 2089, ..., 1666,  693, 2545]),\n",
       " array([1444,  720, 1587, ..., 4701, 2806, 3372]),\n",
       " array([2987, 5541, 5329, ..., 4766, 2563,  246]),\n",
       " array([5783, 5219, 4085, ...,  605,  233, 2680]),\n",
       " array([ 974, 2709, 2339, ..., 3489, 3657, 1020]),\n",
       " array([ 818, 5677, 2255, ..., 2915, 4133, 1860]),\n",
       " array([3174, 2698, 5500, ..., 1483, 4306,  658]),\n",
       " array([4686, 4266, 5948, ...,  760, 3246, 5884]),\n",
       " array([2447, 4351, 3866, ..., 3274, 2591,   58]),\n",
       " array([2072, 4223, 3958, ..., 3407, 5333,  195]),\n",
       " array([1483,   24,  998, ..., 5890, 1928, 2176]),\n",
       " array([5342, 4928, 5901, ..., 2669, 5945, 1498]),\n",
       " array([5993, 4568, 2106, ..., 1784, 3492, 3232]),\n",
       " array([  62,  450,  864, ..., 3515, 5943,  767]),\n",
       " array([ 551, 4844, 1215, ..., 1648, 2259,  438]),\n",
       " array([3682, 2645, 4088, ..., 4821, 3659,  792]),\n",
       " array([4673, 1962, 4673, ..., 1269, 1092, 3880]),\n",
       " array([2507, 5402, 1911, ..., 5619, 5955, 3681]),\n",
       " array([3767, 5352, 2795, ..., 4071,  662, 1979]),\n",
       " array([4858, 3425,  550, ..., 1428, 4234, 4779]),\n",
       " array([3918, 3413, 2320, ..., 5459, 3612, 2417]),\n",
       " array([5863, 1320,    2, ...,  795,  530, 2431]),\n",
       " array([3052, 2844, 5254, ..., 5136,  838, 1355]),\n",
       " array([5334, 5778, 4548, ..., 2270, 1101, 4678]),\n",
       " array([3482,  825, 2232, ..., 5385, 2593, 2989]),\n",
       " array([ 568, 5400, 4723, ..., 3824, 5237, 2792]),\n",
       " array([3189, 1169, 2687, ..., 4677,  378, 2147]),\n",
       " array([3894, 5019,  227, ..., 1840, 4202, 3192]),\n",
       " array([4128, 5998, 4780, ..., 5334, 2422, 2810]),\n",
       " array([3733, 1444, 5423, ..., 1266, 1291, 5081]),\n",
       " array([5716, 2538, 2697, ..., 4785, 1369, 3906]),\n",
       " array([3152, 4234, 1777, ..., 3801, 2411, 2547]),\n",
       " array([ 779, 4320, 5599, ..., 1866, 1926, 1763]),\n",
       " array([5171, 2970, 2762, ..., 4800, 2167, 4321]),\n",
       " array([4999, 2820, 2295, ..., 5992, 1523, 4938]),\n",
       " array([5675, 3900,  756, ..., 3245, 5747, 5342]),\n",
       " array([4762, 1753, 2434, ..., 4985, 5058, 2862]),\n",
       " array([1771, 1338, 4096, ..., 4825, 2557,   69]),\n",
       " array([4386, 5228,  566, ..., 3481, 3624,  661]),\n",
       " array([5916, 2474, 1788, ...,  612, 2115, 1045]),\n",
       " array([3420, 1533,  121, ..., 3574, 3581,  539]),\n",
       " array([ 720,  202, 4648, ..., 4608, 2357,  836]),\n",
       " array([1761, 4898, 2476, ..., 4121, 2261, 3644]),\n",
       " array([1895, 4670,  220, ..., 5208, 4080, 1772]),\n",
       " array([4361, 2273, 1155, ...,  104, 1550,  777]),\n",
       " array([ 354, 2022, 5268, ..., 4975,    7, 3274]),\n",
       " array([ 331, 1914, 2459, ...,  647, 1954,  554]),\n",
       " array([2299, 1356, 2525, ..., 1365, 5801, 3669]),\n",
       " array([1484, 3897,  127, ...,  395, 5489, 5717]),\n",
       " array([ 579, 1221, 5013, ..., 1732, 1493,  807]),\n",
       " array([5259, 1391, 2879, ..., 5971,  907, 5194]),\n",
       " array([3888, 1599, 2341, ..., 5315, 3798, 5296]),\n",
       " array([2201, 1973, 4611, ..., 2645, 1472, 3528]),\n",
       " array([4416,  242,  377, ..., 4176, 4544, 4695]),\n",
       " array([5956, 5164, 1821, ..., 2583, 1582,  853]),\n",
       " array([ 203, 3478, 4036, ..., 3745, 4218, 4345]),\n",
       " array([4445,   57, 1893, ...,  554, 2858,   98]),\n",
       " array([2886,  997, 1585, ..., 4918, 5200, 5264]),\n",
       " array([ 199, 2002, 4990, ..., 5931, 2045, 4041]),\n",
       " array([5067, 4771,  791, ..., 2068, 2349, 2941]),\n",
       " array([5247, 2071, 2188, ..., 2076, 1384, 5021]),\n",
       " array([5431, 1828, 2943, ..., 1840, 4999, 5169]),\n",
       " array([ 986, 3703, 4231, ..., 3995,  448, 5910]),\n",
       " array([4865,  952, 1759, ..., 2474, 3556, 4748])]"
      ]
     },
     "execution_count": 200,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# set a seed for reproducibility\n",
    "np.random.seed(231)\n",
    "\n",
    "n_samples2 = train_df.shape[0]\n",
    "n_B2 = 100\n",
    "\n",
    "# create ten bootstrap samples (will be used to select rows from the DataFrame)\n",
    "samples2 = [np.random.choice(a=n_samples, size=n_samples, replace=True) for _ in range(1, n_B +1 )]\n",
    "samples2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.ensemble import BaggingRegressor\n",
    "from sklearn.ensemble import BaggingClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {},
   "outputs": [],
   "source": [
    "models = {'log': LogisticRegression(solver='liblinear',C=1e9, random_state=123),\n",
    "          'dt1': DecisionTreeClassifier(max_depth=None, random_state=125),\n",
    "          'dt2': DecisionTreeClassifier(max_depth=2, random_state=213)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = pd.DataFrame (columns =models.keys(),index= y_test.index )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {},
   "outputs": [],
   "source": [
    "#for tp in models.keys():\n",
    "for i, model  in models.items():\n",
    "       \n",
    "    bagreg = BaggingClassifier(model, n_estimators=100,bootstrap=True, oob_score=True, random_state=1,n_jobs=-1)\n",
    "    \n",
    "    bagreg.fit(X_train, y_train)\n",
    "    \n",
    "    y_pred[i] = bagreg.predict(X_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>log</th>\n",
       "      <th>dt1</th>\n",
       "      <th>dt2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1483</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2185</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2520</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3721</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3727</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4524</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>234</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4735</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5839</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2939</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3053</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>867</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>276</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5798</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3512</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5789</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3198</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5908</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5718</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2687</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>250</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5461</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3052</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2711</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3771</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5988</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5645</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4617</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>368</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4916</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4088</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1395</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2576</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2878</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3147</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4513</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1620</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4522</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5485</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2394</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1691</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4949</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2805</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>494</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5287</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3316</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3176</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2372</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5947</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5782</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1814</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4704</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5333</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>946</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3077</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5166</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2227</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5684</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1937</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1500 rows Ã— 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      log  dt1  dt2\n",
       "1483    1    1    1\n",
       "2185    1    1    1\n",
       "2520    1    1    1\n",
       "3721    1    1    1\n",
       "3727    0    0    0\n",
       "4524    0    1    1\n",
       "234     0    0    0\n",
       "4735    0    0    0\n",
       "5839    0    0    0\n",
       "2939    1    0    1\n",
       "3053    1    1    1\n",
       "867     1    1    1\n",
       "276     1    1    1\n",
       "5798    0    0    0\n",
       "3512    1    1    1\n",
       "5789    0    0    0\n",
       "3198    1    1    1\n",
       "5908    1    1    1\n",
       "5718    0    1    0\n",
       "2687    0    0    0\n",
       "250     0    1    1\n",
       "5461    1    1    1\n",
       "3052    0    0    0\n",
       "2711    0    1    1\n",
       "3771    0    0    0\n",
       "5988    0    0    0\n",
       "5645    0    1    0\n",
       "4617    0    0    0\n",
       "368     1    0    1\n",
       "4916    0    0    0\n",
       "...   ...  ...  ...\n",
       "19      0    0    0\n",
       "4088    1    1    1\n",
       "1395    1    1    1\n",
       "2576    0    0    0\n",
       "2878    0    0    0\n",
       "3147    0    0    0\n",
       "4513    0    0    0\n",
       "1620    0    1    1\n",
       "4522    1    1    1\n",
       "5485    1    1    1\n",
       "2394    0    0    0\n",
       "1691    0    0    0\n",
       "4949    0    0    0\n",
       "2805    0    1    1\n",
       "494     1    1    1\n",
       "5287    0    0    0\n",
       "3316    0    0    1\n",
       "3176    1    0    1\n",
       "2372    1    1    1\n",
       "5947    1    1    1\n",
       "5782    0    0    0\n",
       "1814    0    0    0\n",
       "4704    0    0    0\n",
       "5333    0    0    0\n",
       "946     0    1    1\n",
       "3077    0    1    1\n",
       "5166    0    0    0\n",
       "2227    1    1    1\n",
       "5684    0    0    0\n",
       "1937    0    0    0\n",
       "\n",
       "[1500 rows x 3 columns]"
      ]
     },
     "execution_count": 205,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LogReg\n",
      "Accuracy Score: 0.625\n",
      "F1 Score:0.607\n",
      " \n",
      "Tree1 max_depth=None\n",
      "Accuracy Score: 0.639\n",
      "F1 Score:0.636\n",
      " \n",
      "Tree1 max_depth=2\n",
      "Accuracy Score: 0.645\n",
      "F1 Score:0.643\n"
     ]
    }
   ],
   "source": [
    "print('LogReg')\n",
    "print(f\"Accuracy Score: {round(accuracy_score(y_test, y_pred.iloc[:,0]),3)}\")\n",
    "print(f\"F1 Score:{round(f1_score(y_test,y_pred.iloc[:,0]),3)}\")\n",
    "print(' ')\n",
    "print('Tree1 max_depth=None')\n",
    "print(f\"Accuracy Score: {round(accuracy_score(y_test, y_pred.iloc[:,1]),3)}\")\n",
    "print(f\"F1 Score:{round(f1_score(y_test,y_pred.iloc[:,1]),3)}\")\n",
    "print(' ')\n",
    "print('Tree1 max_depth=2')\n",
    "print(f\"Accuracy Score: {round(accuracy_score(y_test, y_pred.iloc[:,2]),3)}\")\n",
    "print(f\"F1 Score:{round(f1_score(y_test,y_pred.iloc[:,2]),3)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 9.3\n",
    "\n",
    "Ensemble using majority voting\n",
    "\n",
    "Evaluate using the following metrics:\n",
    "* Accuracy\n",
    "* F1-Score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "votClass=VotingClassifier(estimators=[('lr',LogisticRegression(random_state=7351)),\n",
    "                                     ('trcl',DecisionTreeClassifier(random_state=7351)),\n",
    "                                     ('lr_l1',LogisticRegression(random_state=7351,penalty='l1')),\n",
    "                                     ('lr_l2',LogisticRegression(random_state=7351,penalty='l2'))],\n",
    "                                     \n",
    "                          n_jobs=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "VotingClassifier(estimators=[('lr', LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "          intercept_scaling=1, max_iter=100, multi_class='warn',\n",
       "          n_jobs=None, penalty='l2', random_state=7351, solver='warn',\n",
       "          tol=0.0001, verbose=0, warm_start=False)), ('trcl', Decisi...penalty='l2', random_state=7351, solver='warn',\n",
       "          tol=0.0001, verbose=0, warm_start=False))],\n",
       "         flatten_transform=None, n_jobs=-1, voting='hard', weights=None)"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "votClass.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1 Score:0.605\n",
      "Accuracy Score:0.631\n"
     ]
    }
   ],
   "source": [
    "y_pred=votClass.predict(X_test)\n",
    "print(f\"F1 Score:{round(f1_score(y_test,y_pred),3)}\")\n",
    "print(f\"Accuracy Score:{round(accuracy_score(y_test,y_pred),3)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 9.4\n",
    "\n",
    "Estimate te probability as %models that predict positive\n",
    "\n",
    "Modify the probability threshold and select the one that maximizes the F1-Score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "prob         0.40\n",
       "f1_score     0.70\n",
       "Acc_score    0.64\n",
       "Name: 3, dtype: float64"
      ]
     },
     "execution_count": 135,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf=BaggingClassifier(base_estimator=DecisionTreeClassifier(max_depth=2, random_state=123),\n",
    "                     n_estimators=100, bootstrap=True, random_state=42, n_jobs=-1, oob_score=True)\n",
    "f=[]\n",
    "rango=np.arange(0.1,1,0.1)\n",
    "for j in rango:\n",
    "    clf.fit(X_train, y_train)\n",
    "    y_pred=(clf.predict_proba(X_test)[:,1]>=j).astype(int)\n",
    "    f.append([j,round(f1_score(y_pred, y_test),2), round(clf.score(X_test,y_test),2)])\n",
    "    \n",
    "f=pd.DataFrame(f, columns=['prob', 'f1_score', 'Acc_score'])\n",
    "f.loc[f['f1_score'].idxmax()]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 9.5\n",
    "\n",
    "Ensemble using weighted voting using the oob_error\n",
    "\n",
    "Evaluate using the following metrics:\n",
    "* Accuracy\n",
    "* F1-Score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[DecisionTreeClassifier(class_weight=None, criterion='gini', max_depth=2,\n",
      "            max_features=None, max_leaf_nodes=None,\n",
      "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
      "            min_samples_leaf=1, min_samples_split=2,\n",
      "            min_weight_fraction_leaf=0.0, presort=False, random_state=123,\n",
      "            splitter='best'), DecisionTreeClassifier(class_weight=None, criterion='gini', max_depth=None,\n",
      "            max_features=None, max_leaf_nodes=None,\n",
      "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
      "            min_samples_leaf=1, min_samples_split=2,\n",
      "            min_weight_fraction_leaf=0.0, presort=False, random_state=123,\n",
      "            splitter='best'), LogisticRegression(C=1000000000.0, class_weight=None, dual=False,\n",
      "          fit_intercept=True, intercept_scaling=1, max_iter=100,\n",
      "          multi_class='warn', n_jobs=None, penalty='l2', random_state=123,\n",
      "          solver='liblinear', tol=0.0001, verbose=0, warm_start=False)]\n"
     ]
    }
   ],
   "source": [
    "models=[DecisionTreeClassifier(max_depth=2, random_state=123),DecisionTreeClassifier(max_depth=None, random_state=123),\n",
    "        LogisticRegression(solver='liblinear',C=1e9, random_state=123),]\n",
    "print(models)\n",
    "\n",
    "y_pred_mean=[]\n",
    "fs=[]\n",
    "j=1\n",
    "for model in list(models):\n",
    "    clf = BaggingClassifier(base_estimator=model,n_estimators=100, bootstrap=True, random_state=42, n_jobs=-1, oob_score=True)\n",
    "    clf.fit(X_train,y_train)\n",
    "    errors=np.zeros(clf.n_estimators)\n",
    "    y_pred_all_=np.zeros((X_test.shape[0],clf.n_estimators))\n",
    "    for b in range(clf.n_estimators):\n",
    "        oob_sample = ~clf.estimators_samples_[b]\n",
    "        y_pred_=clf.estimators_[b].predict(pd.DataFrame(X_train).values[oob_sample])\n",
    "        errors[b]=metrics.accuracy_score(y_pred_, y_train.values[oob_sample])\n",
    "        y_pred_all_[:,b]=clf.estimators_[b].predict(X_test)\n",
    "    alpha=(1-errors)/ (1-errors).sum()\n",
    "    y_pred=(np.sum(y_pred_all_*alpha, axis=1)>=0.5).astype(np.int)\n",
    "    y_pred_mean.append([b, y_pred.mean()])\n",
    "    fs.append([j,round(f1_score(y_pred,y_test),3),round(clf.score(X_test,y_test),3)])\n",
    "    j=j+1\n",
    "y_pred_mean=pd.DataFrame(y_pred_mean)\n",
    "y_pred_mean[1].tolist()\n",
    "fs = pd.DataFrame(fs, columns=['prob','f1_score','tree_score'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>prob</th>\n",
       "      <th>f1_score</th>\n",
       "      <th>tree_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0.645</td>\n",
       "      <td>0.645</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>0.651</td>\n",
       "      <td>0.640</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>0.611</td>\n",
       "      <td>0.626</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   prob  f1_score  tree_score\n",
       "0     1     0.645       0.645\n",
       "1     2     0.651       0.640\n",
       "2     3     0.611       0.626"
      ]
     },
     "execution_count": 186,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 9.6\n",
    "\n",
    "Estimate te probability of the weighted voting\n",
    "\n",
    "Modify the probability threshold and select the one that maximizes the F1-Score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.49466666666666664\n"
     ]
    }
   ],
   "source": [
    "y_pred_0= np.asarray(y_pred_df[0])\n",
    "prob_mod1=np.sum(y_pred_0)/y_pred_0.shape[1]\n",
    "print(prob_mod1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[DecisionTreeClassifier(class_weight=None, criterion='gini', max_depth=None,\n",
      "            max_features=None, max_leaf_nodes=None,\n",
      "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
      "            min_samples_leaf=1, min_samples_split=2,\n",
      "            min_weight_fraction_leaf=0.0, presort=False, random_state=123,\n",
      "            splitter='best'), DecisionTreeClassifier(class_weight=None, criterion='gini', max_depth=2,\n",
      "            max_features=None, max_leaf_nodes=None,\n",
      "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
      "            min_samples_leaf=1, min_samples_split=2,\n",
      "            min_weight_fraction_leaf=0.0, presort=False, random_state=123,\n",
      "            splitter='best'), LogisticRegression(C=1000000000.0, class_weight=None, dual=False,\n",
      "          fit_intercept=True, intercept_scaling=1, max_iter=100,\n",
      "          multi_class='warn', n_jobs=None, penalty='l2', random_state=123,\n",
      "          solver='liblinear', tol=0.0001, verbose=0, warm_start=False)]\n",
      "   prob  F1_score  tree_score\n",
      "0     1     0.699        0.64\n",
      "1     2     0.697        0.64\n",
      "2     3     0.651        0.64\n",
      "3     4     0.528        0.64\n",
      "4     5     0.321        0.64\n",
      "5     6     0.122        0.64\n",
      "6     7     0.016        0.64\n",
      "prob          1.000\n",
      "F1_score      0.699\n",
      "tree_score    0.640\n",
      "Name: 0, dtype: float64\n",
      "   prob  F1_score  tree_score\n",
      "0     8     0.647       0.645\n",
      "1     9     0.645       0.645\n",
      "2    10     0.645       0.645\n",
      "3    11     0.644       0.645\n",
      "4    12     0.637       0.645\n",
      "5    13     0.606       0.645\n",
      "6    14     0.579       0.645\n",
      "prob          8.000\n",
      "F1_score      0.647\n",
      "tree_score    0.645\n",
      "Name: 0, dtype: float64\n",
      "   prob  F1_score  tree_score\n",
      "0    15     0.621       0.626\n",
      "1    16     0.613       0.626\n",
      "2    17     0.611       0.626\n",
      "3    18     0.598       0.626\n",
      "4    19     0.589       0.626\n",
      "5    20     0.579       0.626\n",
      "6    21     0.561       0.626\n",
      "prob          15.000\n",
      "F1_score       0.621\n",
      "tree_score     0.626\n",
      "Name: 0, dtype: float64\n",
      "[[[...], prob          1.000\n",
      "F1_score      0.699\n",
      "tree_score    0.640\n",
      "Name: 0, dtype: float64], [[...], prob          8.000\n",
      "F1_score      0.647\n",
      "tree_score    0.645\n",
      "Name: 0, dtype: float64], [[...], prob          15.000\n",
      "F1_score       0.621\n",
      "tree_score     0.626\n",
      "Name: 0, dtype: float64]]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "models=[DecisionTreeClassifier(max_depth=None, random_state=123),DecisionTreeClassifier(max_depth=2, random_state=123),\n",
    "        LogisticRegression(solver='liblinear',C=1e9, random_state=123)]\n",
    "print(models)\n",
    "    \n",
    "y_pred_mean=[]\n",
    "y_pred_df=[]\n",
    "fs=[]\n",
    "fk=[]\n",
    "j=1\n",
    "for model in list(models):\n",
    "    clf = BaggingClassifier(base_estimator=model,n_estimators=100, bootstrap=True, random_state=42, n_jobs=-1, oob_score=True)\n",
    "    \n",
    "    ft=[]\n",
    "    rango=np.arange(0.3,1,0.1)\n",
    "    for t in rango:\n",
    "        clf.fit(X_train, y_train)\n",
    "        errors=np.zeros(clf.n_estimators)\n",
    "        y_pred_all_=np.zeros((X_test.shape[0],clf.n_estimators))\n",
    "        for b in range(clf.n_estimators):\n",
    "            oob_sample = ~clf.estimators_samples_[b]\n",
    "            y_pred_=clf.estimators_[b].predict(pd.DataFrame(X_train).values[oob_sample])\n",
    "            errors[b]=metrics.accuracy_score(y_pred_, y_train.values[oob_sample])\n",
    "            y_pred_all_[:,b]=clf.estimators_[b].predict(X_test)\n",
    "        alpha=(1-errors)/ (1-errors).sum()\n",
    "        y_pred=(np.sum(y_pred_all_*alpha, axis=1)>=t).astype(np.int)\n",
    "        y_pred_df.append([y_pred])\n",
    "        y_pred_mean.append([b, y_pred.mean()])\n",
    "        ft.append([j,round(f1_score(y_pred,y_test),3),round(clf.score(X_test,y_test),3)])\n",
    "        j=j+1\n",
    "    ft=pd.DataFrame(ft, columns=['prob','F1_score','tree_score'])\n",
    "    print(ft)\n",
    "    fs=ft.loc[ft['F1_score'].idxmax()]\n",
    "    print(fs)\n",
    "    fk.append([fk,fs])\n",
    "    \n",
    "print(fk)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 9.7\n",
    "\n",
    "Estimate a logistic regression using as input the estimated classifiers\n",
    "\n",
    "Modify the probability threshold such that maximizes the F1-Score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LogisticRegression(C=1000000000.0, class_weight=None, dual=False,\n",
      "          fit_intercept=True, intercept_scaling=1, max_iter=100,\n",
      "          multi_class='warn', n_jobs=None, penalty='l2', random_state=123,\n",
      "          solver='liblinear', tol=0.0001, verbose=0, warm_start=False)]\n",
      "   prob  F1_score  tree_score\n",
      "0     1     0.621       0.626\n",
      "1     2     0.613       0.626\n",
      "2     3     0.611       0.626\n",
      "3     4     0.598       0.626\n",
      "4     5     0.589       0.626\n",
      "5     6     0.579       0.626\n",
      "6     7     0.561       0.626\n",
      "prob          1.000\n",
      "F1_score      0.621\n",
      "tree_score    0.626\n",
      "Name: 0, dtype: float64\n",
      "[[[...], prob          1.000\n",
      "F1_score      0.621\n",
      "tree_score    0.626\n",
      "Name: 0, dtype: float64]]\n"
     ]
    }
   ],
   "source": [
    "models=[LogisticRegression(solver='liblinear',C=1e9, random_state=123)]\n",
    "print(models)\n",
    "    \n",
    "y_pred_mean=[]\n",
    "y_pred_df=[]\n",
    "fs=[]\n",
    "fk=[]\n",
    "j=1\n",
    "for model in list(models):\n",
    "    clf = BaggingClassifier(base_estimator=model,n_estimators=100, bootstrap=True, random_state=42, n_jobs=-1, oob_score=True)\n",
    "    \n",
    "    ft=[]\n",
    "    rango=np.arange(0.3,1,0.1)\n",
    "    for t in rango:\n",
    "        clf.fit(X_train, y_train)\n",
    "        errors=np.zeros(clf.n_estimators)\n",
    "        y_pred_all_=np.zeros((X_test.shape[0],clf.n_estimators))\n",
    "        for b in range(clf.n_estimators):\n",
    "            oob_sample = ~clf.estimators_samples_[b]\n",
    "            y_pred_=clf.estimators_[b].predict(pd.DataFrame(X_train).values[oob_sample])\n",
    "            errors[b]=metrics.accuracy_score(y_pred_, y_train.values[oob_sample])\n",
    "            y_pred_all_[:,b]=clf.estimators_[b].predict(X_test)\n",
    "        alpha=(1-errors)/ (1-errors).sum()\n",
    "        y_pred=(np.sum(y_pred_all_*alpha, axis=1)>=t).astype(np.int)\n",
    "        y_pred_df.append([y_pred])\n",
    "        y_pred_mean.append([b, y_pred.mean()])\n",
    "        ft.append([j,round(f1_score(y_pred,y_test),3),round(clf.score(X_test,y_test),3)])\n",
    "        j=j+1\n",
    "    ft=pd.DataFrame(ft, columns=['prob','F1_score','tree_score'])\n",
    "    print(ft)\n",
    "    fs=ft.loc[ft['F1_score'].idxmax()]\n",
    "    print(fs)\n",
    "    fk.append([fk,fs]) \n",
    "print(fk)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
